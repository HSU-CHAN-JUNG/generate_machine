{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93954520-cdc6-45cf-9b64-1f1f5e1ebde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "from itertools import accumulate\n",
    "# import lpips\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as dset\n",
    "import torchvision\n",
    "from torch import no_grad\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "%run unet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fcf1782-2c1f-43f1-acce-1d5855b8ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_theta_mius(f_theta, f_theta_mius, mu):\n",
    "    with torch.no_grad():\n",
    "        for para in zip(f_theta_mius.parameters(), f_theta.parameters()):\n",
    "           para[0].data = mu*para[0].data + (1-mu)*para[1].data\n",
    "    # return mu*theta_mius + (1-mu)*theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c28f9c70-1377-4f45-a807-04df0122a214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "batch = 128\n",
    "epoch = 500\n",
    "DOWNLOAD_MNIST = False\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device_str = \"cpu\"\n",
    "resize = 32\n",
    "save_name = \"cm_2\"\n",
    "device = torch.device(device_str)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "643b4a41-719c-448d-a1a9-2c8a3057663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm para\n",
    "s1 = 1000\n",
    "s0 = 10\n",
    "mu0 = 0.99\n",
    "K = epoch\n",
    "# N = lambda k: np.ceil(np.sqrt((k / K * ((s1 + 1) ** 2 - s0 ** 2) + s0 ** 2)) - 1) + 1\n",
    "N = lambda k: 80\n",
    "# mu = lambda k: np.exp(s0 * np.log(mu0) / N(k))\n",
    "mu = lambda k: 0.99\n",
    "\n",
    "epsilon = 0.002\n",
    "T = 80\n",
    "ro = 7\n",
    "# time_schedule = lambda i: (epsilon**(1/ro) + (i-1)/(N-1)*(T**(1/ro) - epsilon**(1/ro)))**ro\n",
    "time_schedule = lambda i, k: (epsilon ** (1 / ro) + (i - 1) / (N(k) - 1) *(\n",
    "            T ** (1 / ro) - epsilon ** (1 / ro))) ** ro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71a738b1-132a-4e8e-876d-bece1810762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    # transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    ")\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root = 'dataset/mnist',\n",
    "    train = True,\n",
    "    transform = data_transform, #改成torch可讀\n",
    "    download = DOWNLOAD_MNIST,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b4b8a20-dc84-42c3-964d-2a153848ec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 23.63 GiB of which 77.19 MiB is free. Process 3074860 has 22.50 GiB memory in use. Process 3097273 has 980.00 MiB memory in use. Of the allocated memory 451.90 MiB is allocated by PyTorch, and 62.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m c_skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;241m0.25\u001b[39m \u001b[38;5;241m/\u001b[39m ((t \u001b[38;5;241m-\u001b[39m epsilon) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.25\u001b[39m)\n\u001b[1;32m     50\u001b[0m c_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (t \u001b[38;5;241m-\u001b[39m epsilon) \u001b[38;5;241m/\u001b[39m (t \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.25\u001b[39m)\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[0;32m---> 51\u001b[0m f1 \u001b[38;5;241m=\u001b[39m c_skip(time_next) \u001b[38;5;241m*\u001b[39m Xt_next \u001b[38;5;241m+\u001b[39m c_out(time_next) \u001b[38;5;241m*\u001b[39m \u001b[43mf_theta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt_next\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_schedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mep\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 餵給net吃訓練數據x, 輸出預測值\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m no_grad():\n\u001b[1;32m     54\u001b[0m     f2 \u001b[38;5;241m=\u001b[39m c_skip(time) \u001b[38;5;241m*\u001b[39m Xt \u001b[38;5;241m+\u001b[39m c_out(time) \u001b[38;5;241m*\u001b[39m f_theta_mius(Xt, time_schedule(index, ep)\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m     55\u001b[0m         device))  \u001b[38;5;66;03m# 餵給net吃訓練數據x, 輸出預測值\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/ipykernel_8001/2038583819.py:126\u001b[0m, in \u001b[0;36mUnet.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m#Down\u001b[39;00m\n\u001b[1;32m    125\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown1(x1, t)\n\u001b[0;32m--> 126\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown2(x2, t)\n\u001b[1;32m    128\u001b[0m x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa2(x3)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/ipykernel_8001/2038583819.py:75\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize)\u001b[38;5;241m.\u001b[39mswapaxes(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     74\u001b[0m x_ln \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(x)\n\u001b[0;32m---> 75\u001b[0m attention_value, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_ln\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_ln\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_ln\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m attention_value \u001b[38;5;241m=\u001b[39m attention_value \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m     77\u001b[0m attention_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_self(attention_value) \u001b[38;5;241m+\u001b[39m attention_value\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:5441\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5439\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbaddbmm(attn_mask, q_scaled, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   5440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 5441\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5442\u001b[0m attn_output_weights \u001b[38;5;241m=\u001b[39m softmax(attn_output_weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   5443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout_p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 23.63 GiB of which 77.19 MiB is free. Process 3074860 has 22.50 GiB memory in use. Process 3097273 has 980.00 MiB memory in use. Of the allocated memory 451.90 MiB is allocated by PyTorch, and 62.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainLoader = dset.DataLoader(trainset, batch_size=batch, shuffle=True)\n",
    "lr = 1e-2\n",
    "f_theta = Unet().to(device)\n",
    "f_theta_mius = Unet().to(device)\n",
    "f_theta_mius.load_state_dict(f_theta.state_dict())\n",
    "optimizer = torch.optim.Adam(f_theta.parameters(), lr=lr)\n",
    "loss_func = nn.MSELoss()\n",
    "loss_set = []\n",
    "batch_loss = []\n",
    "\n",
    "### Load model\n",
    "# f_theta = Unet().to(device)\n",
    "# checkpoint = torch.load(f\"weights/eps_theta_{save_name}.pt\", weights_only=False)\n",
    "# f_theta.load_state_dict(checkpoint['model_state_dict'])\n",
    "# f_theta_mius = Unet().to(device)\n",
    "# f_theta_mius.load_state_dict(f_theta.state_dict())\n",
    "# loss_set = checkpoint['loss_set']\n",
    "# optimizer = torch.optim.Adam(f_theta.parameters(), lr=lr)\n",
    "# loss_func = nn.MSELoss()\n",
    "\n",
    "# loss_fn_alex = lpips.LPIPS(net='alex') # best forward scores\n",
    "# loss_fn_vgg = lpips.LPIPS(net='vgg') # closer to \"traditional\" perceptual loss, when used for optimization\n",
    "\n",
    "# image should be RGB, IMPORTANT: normalized to [-1,1]\n",
    "# d = loss_fn_alex(img0, img1)\n",
    "\n",
    "pbar = tqdm(range(epoch))\n",
    "# mu = 0.1\n",
    "# accumulation_steps = 4\n",
    "for ep in pbar:\n",
    "    # for ep in range(epoch):\n",
    "    for i, (X_data, _) in enumerate(trainLoader):\n",
    "        # with torch.autocast(device_type=device_str, dtype=torch.bfloat16):\n",
    "\n",
    "        X_data = X_data.to(device)\n",
    "        # X_data = 2 * X_data.to(device) - 1\n",
    "        # index = torch.randint(N, size=(X_data.shape[0],))\n",
    "        index = torch.randint(int(N(ep)), size=(X_data.shape[0],))\n",
    "\n",
    "        # forward of DDPM\n",
    "        noise = torch.randn_like(X_data)\n",
    "        time_next = time_schedule(index + 1, ep)[:, None, None, None].to(device)\n",
    "        time = time_schedule(index, ep)[:, None, None, None].to(device)\n",
    "        # Xt = X_data + (2 * time).sqrt() * noise\n",
    "\n",
    "        Xt = X_data + time * noise\n",
    "        Xt_next = X_data + time_next * noise\n",
    "\n",
    "        c_skip = lambda t: 0.25 / ((t - epsilon) ** 2 + 0.25)\n",
    "        c_out = lambda t: 0.5 * (t - epsilon) / (t ** 2 + 0.25).sqrt()\n",
    "        f1 = c_skip(time_next) * Xt_next + c_out(time_next) * f_theta(Xt_next, time_schedule(index + 1, ep).to(\n",
    "            device))  # 餵給net吃訓練數據x, 輸出預測值\n",
    "        with no_grad():\n",
    "            f2 = c_skip(time) * Xt + c_out(time) * f_theta_mius(Xt, time_schedule(index, ep).to(\n",
    "                device))  # 餵給net吃訓練數據x, 輸出預測值\n",
    "\n",
    "\n",
    "        loss = 1 * loss_func(f1, f2)\n",
    "\n",
    "        # loss = loss /accumulation_steps\n",
    "        loss.backward()  # 誤差反向傳導\n",
    "        optimizer.step()  # 神經網路參數更新\n",
    "        optimizer.zero_grad()  # 梯度清0\n",
    "        mu_k = mu(ep)\n",
    "        update_theta_mius(f_theta, f_theta_mius, mu_k)\n",
    "\n",
    "\n",
    "        # if (i+1) % accumulation_steps == 0:\n",
    "            # optimizer.step()  # 神經網路參數更新\n",
    "            # optimizer.zero_grad()  # 梯度清0\n",
    "            # mu_k = mu(ep)\n",
    "            # update_theta_mius(f_theta, f_theta_mius, mu_k)\n",
    "        pbar.set_description('Loss: {}'.format(loss.item()))  # 更新pbar\n",
    "        # print('Loss{}: {}'.format(loss.item()))\n",
    "        # lr = 8e-5 if loss<0.02 else 1e-3\n",
    "        loss_set.append(loss.item())\n",
    "\n",
    "iteration = np.arange(len(loss_set))\n",
    "fig = plt.figure()\n",
    "plt.loglog(iteration, loss_set, \"-\")\n",
    "# plt.show()\n",
    "\n",
    "# try:\n",
    "#     checkpoint\n",
    "# except NameError:\n",
    "#     epoch_last = 0\n",
    "# else:\n",
    "#     epoch_last = checkpoint[\"epoch\"]\n",
    "torch.save({\n",
    "    # 'statement':\n",
    "    # f\"This model is U-net structure and DDPM process with linear schedule. \\n\n",
    "    #   It trained for {epoch_last + epoch} epoches with {N} steps. \\n\n",
    "    #   The dictionary save the model, trained parameter of model, Adam optimizer and loss. \\n\n",
    "    #   You can keep training for your goal with the dictionary.\"\n",
    "    'epoch': epoch,\n",
    "    # 'step': N,\n",
    "    'model': f_theta,\n",
    "    'model_state_dict': f_theta.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss_set': loss_set\n",
    "}, f\"weights/eps_theta_{save_name}.pt\")\n",
    "iteration = np.arange(len(loss_set))\n",
    "fig = plt.figure()\n",
    "plt.loglog(iteration, loss_set, \"-\")\n",
    "plt.savefig(f\"{save_name}.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ff98d-44df-444d-a552-52f376527b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "Total_data = 50000\n",
    "# batch = 64#5000\n",
    "dtype = torch.float32\n",
    "Total_iter = Total_data // batch + (Total_data % batch != 0)\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model_weight_name = \"cm_cifar10_6\"\n",
    "device = torch.device(device_str)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3453bce-0f74-473c-9e44-ca4ad240f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_theta = Unet().to(device)\n",
    "checkpoint = torch.load(f\"weights/eps_theta_{save_name}.pt\", weights_only=False)\n",
    "f_theta.load_state_dict(checkpoint['model_state_dict'])\n",
    "f_theta = f_theta.eval()\n",
    "f_theta = torch.compile(f_theta.to(device, dtype=dtype),fullgraph=True)\n",
    "loss_set = checkpoint['loss_set']\n",
    "print(len(loss_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73017b1d-b6c5-413c-899d-92f0a2658d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pbar = tqdm(range(Total_iter))\n",
    "pbar = tqdm(range(1))\n",
    "for k in pbar:\n",
    "    ep = 1000\n",
    "    stard_id = k * batch\n",
    "    # index = torch.randint(int(N(ep)), size=(int(batch),))\n",
    "    end_id = min((k + 1) * batch, Total_data)\n",
    "    num_of_data = end_id - stard_id\n",
    "    # backward process\n",
    "    with torch.inference_mode(), torch.amp.autocast(enabled=True, device_type=device_str, dtype=torch.bfloat16):\n",
    "        #with torch.amp.autocast():\n",
    "        x_hat = torch.randn(num_of_data, 1, 32, 32,device=device, dtype=dtype) *T\n",
    "        T_time = torch.ones(len(x_hat), device=device, dtype=dtype) * T\n",
    "    \n",
    "        c_skip = lambda t: 0.25 / ((t - epsilon) ** 2 + 0.25)\n",
    "        c_out = lambda t: np.sqrt(0.5 * (t - epsilon) / (t ** 2 + 0.25))\n",
    "        x = c_skip(T) * x_hat + c_out(T) * f_theta(x_hat, T_time)\n",
    "        # x = 0.5*x + 0.5\n",
    "    \n",
    "    # x_motion = torch.hstack([x_motion, x.unsqueeze(-1)])\n",
    "    \n",
    "    # result.append(x)   # Save the result\n",
    "    resize_f = transforms.Resize((28,28))\n",
    "    x_temp = resize_f(x).to(device=\"cpu\", dtype=torch.float32).numpy().transpose((0, 2, 3, 1))  # [0][999]/\n",
    "    x_temp = x.to(device=\"cpu\", dtype=torch.float32).numpy().transpose((0, 2, 3, 1))  # [0][999]\n",
    "    x_temp = np.clip(x_temp, 0.0, 1.0)\n",
    "    # results.append((x_temp * 255).astype(np.uint8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4cd5a3-d106-41ad-93fd-f09502dd6a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 80\n",
    "\n",
    "num_fig = 8\n",
    "steps = int(N/5)\n",
    "num_steps = int(N/steps)+1\n",
    "# N=1000\n",
    "\n",
    "fig = plt.figure(figsize=(num_fig, num_fig))\n",
    "\n",
    "for j in range(num_fig**2):\n",
    "    plt.subplot(num_fig, num_fig, j+1)     #将窗口分为两行两列四个子图，则可显示四幅图片\n",
    "    # plt.title('Step = {}'.format(N - i))   #第一幅图片标题\n",
    "    plt.imshow(x_temp[j], cmap = \"gray\")      #绘制第一幅图片\n",
    "    plt.axis(\"off\")\n",
    "    k = k+1\n",
    "    # plt.pause(1)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(wspace = 0, hspace = 0)\n",
    "\n",
    "# plt.show()   #显示窗口\n",
    "# plt.savefig(\"SM_sample_8\")   #显示窗口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8777ed9-eae2-495a-b926-3287887c5ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reverse process\n",
    "\n",
    "num_fig = 4\n",
    "steps = 200\n",
    "num_steps = int(N(0)/steps)+1\n",
    "\n",
    "plt.figure(figsize=(2*num_steps, 2*num_fig))\n",
    "\n",
    "for j in range(num_fig):\n",
    "    k = 0\n",
    "    for i in range(0, N(0)+1, steps):\n",
    "        plt.subplot(num_fig, num_steps, k + num_steps*j+1)     #将窗口分为两行两列四个子图，则可显示四幅图片\n",
    "        plt.title('Step = {}'.format(N(0) - i))   #第一幅图片标题\n",
    "        plt.imshow(x_motion[j][i], cmap = \"gray\")      #绘制第一幅图片\n",
    "        plt.axis(\"off\")\n",
    "        k = k+1\n",
    "        # plt.pause(1)\n",
    "\n",
    "\n",
    "# plt.show()   #显示窗口\n",
    "# plt.savefig(\"SM_sample_8\")   #显示窗口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6524d-cba2-4485-abc6-b2a93f5ef443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
